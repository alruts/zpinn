{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.01592879, -0.01134333],\n",
       "       [ 0.01592879, -0.01134333],\n",
       "       [ 0.01592879, -0.01134333],\n",
       "       [ 0.01592879, -0.01134333]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.zpinn.models.SIREN import SIREN\n",
    "from src.zpinn.models.SIREN import SIREN\n",
    "from src.zpinn.modules.sine_layer import SineLayer\n",
    "\n",
    "model_key = jrandom.PRNGKey(0)\n",
    "\n",
    "model = SIREN(\n",
    "    key=model_key,\n",
    "    in_features=4,\n",
    "    out_features=2,\n",
    "    hidden_features=256,\n",
    "    outermost_linear=True,\n",
    "    hidden_layers=3,\n",
    "    first_omega_0=30.0,\n",
    "    hidden_omega_0=30.0,\n",
    ")\n",
    "\n",
    "# test forward pass\n",
    "val = model(*tuple([0.0] * 4))\n",
    "\n",
    "\n",
    "val = jax.vmap(model, in_axes=(0, 0, 0, 0))(jnp.zeros((4)), jnp.zeros((4)), jnp.zeros((4)), jnp.zeros((4)))\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m im, re \u001b[38;5;241m=\u001b[39m val\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "im, re = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01592879 -0.011343336\n"
     ]
    }
   ],
   "source": [
    "a, b = val\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(params, model, *args):\n",
    "    \"\"\"Forward pass of the model as a function of its parameters.\"\"\"\n",
    "\n",
    "    get_params = lambda m: m.get_params()\n",
    "    model = eqx.tree_at(get_params, model, params)\n",
    "    return model(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.01592879, -0.01134334], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_model(model.get_params(), model, *tuple([0.0] * 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, 0.0, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tuple = (None, None, *[0.0] * 4)\n",
    "result_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SIREN' object has no attribute 'get_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m y \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m,))\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Compute the gradient of each loss w.r.t. the parameters\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjacrev(total_loss_fn, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m(), model, x, y)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Compute the grad norm of each loss\u001b[39;00m\n\u001b[0;32m     35\u001b[0m grad_norm_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SIREN' object has no attribute 'get_params'"
     ]
    }
   ],
   "source": [
    "from jax.tree_util import tree_map, tree_leaves, tree_reduce\n",
    "from jax.flatten_util import ravel_pytree\n",
    "\n",
    "flatten_pytree = lambda pytree: ravel_pytree(pytree)[0]\n",
    "\n",
    "def physics_loss_fn(params, model, x, y):\n",
    "    y_pred = apply_model(params, model, *x)\n",
    "    return jnp.mean(jnp.square(y - y_pred))\n",
    "\n",
    "\n",
    "def bc_loss_fn(params, model, x, y):\n",
    "    y_pred = apply_model(params, model, *x)\n",
    "    return jnp.mean(jnp.square(y - y_pred))\n",
    "\n",
    "\n",
    "def data_loss_fn(params, model, x, y):\n",
    "    y_pred = apply_model(params, model, *x)\n",
    "    return jnp.mean(jnp.square(y - y_pred))\n",
    "\n",
    "\n",
    "def total_loss_fn(params, model, x, y):\n",
    "    return dict(\n",
    "        physics_loss=physics_loss_fn(params, model, x, y),\n",
    "        bc_loss=bc_loss_fn(params, model, x, y),\n",
    "        data_loss=data_loss_fn(params, model, x, y),\n",
    "    )\n",
    "    \n",
    "x = tuple([0.0] * 4)\n",
    "y = jnp.zeros((2,))\n",
    "\n",
    "# Compute the gradient of each loss w.r.t. the parameters\n",
    "grads = jax.jacrev(total_loss_fn, argnums=0)(model.get_params(), model, x, y)\n",
    "\n",
    "# Compute the grad norm of each loss\n",
    "grad_norm_dict = {}\n",
    "for key, value in grads.items():\n",
    "    flattened_grad = flatten_pytree(value)\n",
    "    grad_norm_dict[key] = jnp.linalg.norm(flattened_grad)\n",
    "\n",
    "\n",
    "# Compute the mean of grad norms over all losses\n",
    "mean_grad_norm = jnp.mean(jnp.stack(tree_leaves(grad_norm_dict)))\n",
    "\n",
    "# Grad Norm Weighting\n",
    "w = tree_map(lambda x: (mean_grad_norm / x), grad_norm_dict)\n",
    "w\n",
    "\n",
    "# add losses together\n",
    "loss = tree_reduce(lambda x, y: x * y, w) \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1530294719.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    if epoch % update_loss_weights_every_iter == 0:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "'Adaptive loss weighting'\n",
    "            if epoch % update_loss_weights_every_iter == 0:\n",
    "                \n",
    "                # All parameters in the MLP\n",
    "                if pnet_helmholtz.__class__.__name__ == 'MLP' or pnet_helmholtz.__class__.__name__ == 'ModMLP':\n",
    "                    thetas = list(map(lambda layer: layer.weight, pnet_helmholtz.linears)) \n",
    "\n",
    "                # Calculate derivative with respect to parameters\n",
    "                loss_data_grads = torch.autograd.grad(loss_data, thetas, torch.ones_like(loss_data), retain_graph=True)[0]  # \n",
    "                loss_r_grads = torch.autograd.grad(loss_r, thetas, torch.ones_like(loss_r), retain_graph=True)[0]\n",
    "                loss_std_grads = list(map(lambda l: torch.autograd.grad(l, thetas, torch.ones_like(l), retain_graph = True)[0], loss_std))    \n",
    "               \n",
    "\n",
    "                lambda_data, lambda_residual,lambda_std,loss_logger = calc_loss_weights(loss_logger,loss_data_grads, loss_r_grads, loss_std_grads, \n",
    "                                                                            lambda_data, lambda_residual, lambda_std,\n",
    "                                                                             alpha=alpha_lambda, alpha_std=alpha_lambda_std)\n",
    "\n",
    "\n",
    "def calc_loss_weights(loss_logger,loss_data_grad, loss_residual_grad, loss_std_grad, \n",
    "                      lambda_data_old, lambda_residual_old,lambda_std_old, alpha, alpha_std):\n",
    "    # Calculate the dynamic loss weightening as proposed in \n",
    "    # - \"Understanding and mitigating gradient flow pathologies in physics-informed neural networks\", S. Wang et al., and \n",
    "    # - \"An experts's guide to training physics-informed neural networks\", S. Wang et al\n",
    "    \n",
    "\n",
    "    # Calculate the weights from the relative change between losses\n",
    "    'Scheme 1: Mean-based weighting scheme'\n",
    "    data_metrics = torch.norm(loss_data_grad, p=2) \n",
    "    res_metrics = torch.norm(loss_residual_grad, p=2)\n",
    "    std_metrics = list(map(lambda l: torch.norm(l, p=2), loss_std_grad))\n",
    "\n",
    "\n",
    "    'Scheme 2: STD based weighting scheme'\n",
    "    # data_metrics = torch.std(loss_data_grad) \n",
    "    # res_metrics = torch.std(loss_residual_grad)\n",
    "    # std_metrics = list(map(lambda l: torch.std(l), loss_std_grad))\n",
    "\n",
    "    'Calculate weights'\n",
    "    lambda_data_temp = data_metrics + res_metrics/data_metrics\n",
    "    lambda_r_temp = data_metrics + res_metrics/res_metrics\n",
    "    lambda_std_temp = list(map(lambda l: data_metrics + res_metrics/l, std_metrics))\n",
    "\n",
    "    'Scheme 3: Kurtosis and standard deviation-based'\n",
    "    # lambda_data_temp = kurtosis(loss_data_grad)/torch.std(loss_data_grad)\n",
    "    # lambda_r_temp = kurtosis(loss_residual_grad)/torch.std(loss_residual_grad)\n",
    "    # lambda_std_temp = list(map(lambda l: kurtosis(l)/torch.std(l), loss_std_grad))\n",
    "\n",
    "    # moving average\n",
    "    lambda_data = alpha*lambda_data_old + (1-alpha)*lambda_data_temp\n",
    "    lambda_r = alpha*lambda_residual_old + (1-alpha)*lambda_r_temp\n",
    "    lambda_std = [alpha_std*lambda_std_old[i] + (1-alpha_std)*lambda_std_temp[i] for i in range(len(lambda_std_old))]\n",
    "\n",
    "    # Add to loss logger\n",
    "    loss_logger.set_weights(data_metrics, res_metrics, std_metrics,lambda_data_temp,lambda_r_temp,lambda_std_temp,\n",
    "                              lambda_data,lambda_r,lambda_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.zpinn.dataio import PressureDataset\n",
    "\n",
    "dataset = PressureDataset(\n",
    "    path=r\"C:\\Users\\STNj\\dtu\\thesis\\code\\data\\processed\\inf_baffle.pkl\",\n",
    ")\n",
    "\n",
    "dataloader = dataset.get_dataloader(batch_size=32, shuffle=True)\n",
    "\n",
    "f, x, y, z = next(iter(dataloader))[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "t = (1,2,3,4)\n",
    "\n",
    "def test(*args):\n",
    "    print(args)\n",
    "\n",
    "def nested_test(*args):\n",
    "    test(*args)\n",
    "    \n",
    "nested_test(*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real part:\n",
      "2.0*pi*f*rho_0*z_c*(-a_c*d_re*(b_0 + b_c*p_im) + b_c*d_im*(a_0 + a_c*p_re))/(a_c**2*d_re**2 + 1.0*b_c**2*d_im**2)\n",
      "\n",
      "Imaginary part:\n",
      "2.0*pi*f*rho_0*z_c*(a_c*d_re*(a_0 + a_c*p_re) + b_c*d_im*(b_0 + b_c*p_im))/(a_c**2*d_re**2 + 1.0*b_c**2*d_im**2)\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, re, im, diff, pi, I\n",
    "\n",
    "# Define symbols\n",
    "p_re, p_im, dp_re, dp_im, a_c, a_0, b_c, b_0, f, f_c, f_0, z_c, rho_0, z = symbols(\n",
    "    \"p_re p_im d_re d_im a_c a_0 b_c b_0 f f_c f_0 z_c rho_0 z\", real=True\n",
    ")\n",
    "\n",
    "# Define the expression\n",
    "\n",
    "p = (p_re *a_c + a_0) + 1j*(p_im *b_c + b_0)\n",
    "\n",
    "un = 1 / (1j * 2 * pi * f * rho_0) / z_c\n",
    "un *=  (a_c * dp_re + 1j*b_c * dp_im) \n",
    "\n",
    "Z_n = p / un\n",
    "\n",
    "# Extract real and imaginary parts\n",
    "real_part = re(Z_n).simplify()\n",
    "imag_part = im(Z_n).simplify()\n",
    "\n",
    "print(\"Real part:\")\n",
    "print(real_part)\n",
    "print(\"\\nImaginary part:\")\n",
    "print(imag_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{2.0 \\pi f \\rho_{0} z_{c} \\left(a_{0} b_{c} d_{im} - a_{c} b_{0} d_{re} + a_{c} b_{c} d_{im} p_{re} - a_{c} b_{c} d_{re} p_{im}\\right)}{1.0 a_{c}^{2} d_{re}^{2} + 1.0 b_{c}^{2} d_{im}^{2}}$"
      ],
      "text/plain": [
       "2.0*pi*f*rho_0*z_c*(a_0*b_c*d_im - a_c*b_0*d_re + a_c*b_c*d_im*p_re - a_c*b_c*d_re*p_im)/(1.0*a_c**2*d_re**2 + 1.0*b_c**2*d_im**2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_part.factor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random, pmap, local_device_count\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BaseSampler(Dataset):\n",
    "    def __init__(self, batch_size, rng_key=random.PRNGKey(1234)):\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "        self.num_devices = local_device_count()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generate one batch of data\"\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        keys = random.split(subkey, self.num_devices)\n",
    "        batch = self.data_generation(keys)\n",
    "        return batch\n",
    "\n",
    "    def data_generation(self, key):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "\n",
    "class UniformSampler(BaseSampler):\n",
    "    def __init__(self, dom, batch_size, rng_key=random.PRNGKey(1234)):\n",
    "        super().__init__(batch_size, rng_key)\n",
    "        self.dom = dom\n",
    "        self.dim = dom.shape[0]\n",
    "\n",
    "    @partial(pmap, static_broadcasted_argnums=(0,))\n",
    "    def data_generation(self, key):\n",
    "        \"Generates data containing batch_size samples\"\n",
    "        batch = random.uniform(\n",
    "            key,\n",
    "            shape=(self.batch_size, self.dim),\n",
    "            minval=self.dom[:, 0],\n",
    "            maxval=self.dom[:, 1],\n",
    "        )\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    \n",
    "sampler = UniformSampler(dom=jnp.array([[0.0, 1.0]]), batch_size=32)\n",
    "it = iter(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCLGen\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generator for collocation points.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m            Dictionary containing the distributions for the points 'uniform' or 'grid'.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     num_points: \u001b[38;5;28mint\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.09473562],\n",
       "        [0.80631065],\n",
       "        [0.96966016],\n",
       "        [0.7157742 ],\n",
       "        [0.07006764],\n",
       "        [0.6013645 ],\n",
       "        [0.7976681 ],\n",
       "        [0.03148794],\n",
       "        [0.55536425],\n",
       "        [0.6960144 ],\n",
       "        [0.5531845 ],\n",
       "        [0.9130745 ],\n",
       "        [0.8807967 ],\n",
       "        [0.82231283],\n",
       "        [0.6479956 ],\n",
       "        [0.1361841 ],\n",
       "        [0.08883166],\n",
       "        [0.00661266],\n",
       "        [0.09548712],\n",
       "        [0.34428847],\n",
       "        [0.04157531],\n",
       "        [0.33518732],\n",
       "        [0.33928144],\n",
       "        [0.58889234],\n",
       "        [0.34694982],\n",
       "        [0.20022178],\n",
       "        [0.23062909],\n",
       "        [0.18157935],\n",
       "        [0.62718284],\n",
       "        [0.45432687],\n",
       "        [0.5500257 ],\n",
       "        [0.1126709 ]]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyimpedance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
